<!DOCTYPE html>
<html>
  <head>
    <title>MUTAG Classification</title>
    <link rel="stylesheet" type="text/css" href="./index.css">
    <script src="https://d3js.org/d3.v5.js"></script>
  </head>
  <body>
    <header class="stone-background">
      <div class="vertical-padding">
	<h1 style="">Chemical Compound Classification</h1>
	<p>Neural classification of chemical compounds according to their mutagenic effect on bacteria using graph2vec.</p>
      </div>
    </header>
    <section id="introduction">
      <div class="horizontal-padding vertical-padding">
	<h3>Introduction</h3>
	<p>This article goes over our findings for classifying chemical compounds based on their mutagenic affect on a particular bacterium.</p>
	<p>This effort was motivated by a desire to verify the validity of the results from <a target="_blank" href="https://arxiv.org/abs/1707.05005">graph2vec</a>. Rather than directly reproducing the results by using an SVM classifier, we chose to use a deep feedforward neural network.</p>
	<p>We use the MUTAG dataset. It can be found <a target="_blank" href="https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets">here</a>.</p>
	<p>We were able to achieve similar and sometimes better results and give an explanation of this in our <a href="#experiment-results">result summary</a> found later in this article.</p>
      </div>
    </section>
    <section id="experiment-overview" class="stone-background">
      <div class="horizontal-padding vertical-padding">
	<h3>Experiment Overview</h3>
	<p>The MUTAG dataset contains 188 chemical compounds that are labelled according to their mutagenic effect on a bacterium. </p>
	<p>The <a target="_blank" href="https://arxiv.org/abs/1707.05005">graph2vec paper</a> split the data to use 90% for training and 10% for testing. We similarly used 10% for testing, 10% for validation, and 80% for training.</p>
	<p>Here's a depiction of our model.</p>
	<div class="architecture-depiction-container svg-container-center">
	  <svg id="neural-classifier-depiction"></svg>
	</div>
	<p>Dropout was applied after every dense layer.</p>
	<p>The rounded result (either zero or one) was used to determine our final result. This means we used a threshold of 0.5. We did not optimizer this threshold. </p>
	<p>We used binary cross entropy as our loss function and the Adam optimizer. Rounding to get the final result did not happen during training (we simply used the result from the sigmoid).</p>
	<p>The hyperparameters for our model were:</p>
	<ul>
	  <li>graph2vec Rooted Subgraph Size</li>
	  <li>graph2vec Embedding Size</li>
	  <li>Number of graph2vec Training Epochs</li>
	  <li>graph2vec Learning Rate (used solely for graph2vec training)</li>
	  <li>Batch Size</li>
	  <li>Neural Learning Rate (used to train all parameters outside of graph2vec)</li>
	  <li>Number of Dense Layers</li>
	  <li>Gradient Clipping Threshold</li>
	  <li>Dropout Probability</li>
	</ul>
	<p>We first trained our graph2vec embeddings and then trained the model itself independently (with the embedding vectors frozen).</p>
	<p>We used early stopping when training the neural classifier, but a fixed number of epochs for training our graph2vec embeddings.</p>
	<p>Included in the dense layer count selection was the option to use zero dense layers, which turns our model into a logistic regression model.</p>
	<p>We tuned our hyperparameters using two methods, via <a target="_blank" href="https://en.wikipedia.org/wiki/Hyperparameter_optimization#Random_search">random search</a> and via a <a target="_blank" href="http://proceedings.mlr.press/v28/bergstra13.pdf">tree-structured Parzen estimator</a> with <a target="_blank" href="https://arxiv.org/abs/1502.07943">successive halving</a>.</p>
	<p></p>
      </div>
    </section>
    <section id="experiment-results">
      <div class="horizontal-padding vertical-padding">
	<h3>Experiment Results</h3>
	<p>Below, we'll compare the vanilla LSTM architecture to the LSTM with attention architecture w.r.t. loss minimization, loss minimization per parameter, and training time until convergence.</p>
	<p>There were 468 possible hyperparameter combinations for the vanilla LSTM architecture.</p>
      </div>
    </section>
    <section id="conclusion" class="stone-background">
      <div class="horizontal-padding vertical-padding">
	<h3>Conclusion</h3>
	<p>We've shown that attention mechanisms, though seemingly redundant, can yield better results in practice.</p>
	<p>Hopefully, the work here does not solely convey on the benefits of attention but also conveys the importance of practical experience. Learning through experimentation and practice yield far superior results to textbook study alone.</p>
      </div>
      <table style="table-layout: fixed; width: 100%; padding-top: 40px; padding-bottom: 40px;">
	<tr>
	  <td style="width:10%;"></td>
	  <td style="width:30%;">
      	    <card class="stone-background">
      	      <a target="_blank" href="https://github.com/paul-tqh-nguyen">
      		<div class="card-text">
      		  <p>Interested in my work?</p>
      		  <p><b>See my projects on GitHub.</b></p>
      		</div>
      	      </a>
      	    </card>
	  </td>
	  <td style="width:20%;"></td>
	  <td style="width:30%;">
      	    <card class="stone-background">
      	      <a target="_blank" href="https://paul-tqh-nguyen.github.io/about/">
      		<div class="card-text">
      		  <p>Want to learn more about me?</p>
      		  <p><b>Visit my website.</b></p>
      		</div>
      	      </a>
      	    </card>
	  </td>
	  <td style="width:10%;"></td>
	</tr>
      </table>
    </section>
    <script src="index.js"></script>
  </body>
</html>
